"""
Dataset PyTorch pour les s√©quences MIDI tokeniz√©es

Ce module d√©finit le dataset custom qui charge les s√©quences tokeniz√©es
depuis les fichiers pickle cr√©√©s par le preprocessing.

Architecture:
- MIDIDataset: Dataset PyTorch pour charger les s√©quences
- G√®re le padding si n√©cessaire
- Supporte le masking pour l'attention du Transformer
"""

import pickle
import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import List, Optional
import numpy as np


class MIDIDataset(Dataset):
    """
    Dataset PyTorch pour les s√©quences MIDI tokeniz√©es.
    
    Args:
        sequences_path (Path): Chemin vers le fichier .pkl contenant les s√©quences
        max_seq_len (int): Longueur maximale des s√©quences (pour padding)
        pad_token_id (int): ID du token de padding (par d√©faut 0)
        
    Attributes:
        sequences (List[List[int]]): Liste des s√©quences tokeniz√©es
        max_seq_len (int): Longueur maximale des s√©quences
        pad_token_id (int): Token utilis√© pour le padding
    """
    
    def __init__(
        self, 
        sequences_path: Path, 
        max_seq_len: int = 1024,
        pad_token_id: int = 0
    ):
        """Initialise le dataset."""
        self.sequences_path = sequences_path
        self.max_seq_len = max_seq_len
        self.pad_token_id = pad_token_id
        
        # Charger les s√©quences
        print(f"üìÇ Chargement des s√©quences depuis: {sequences_path}")
        with open(sequences_path, 'rb') as f:
            self.sequences = pickle.load(f)
        
        print(f"‚úì {len(self.sequences)} s√©quences charg√©es")
        
        # Statistiques
        lengths = [len(seq) for seq in self.sequences]
        print(f"  Longueur moyenne: {np.mean(lengths):.1f} tokens")
        print(f"  Longueur min/max: {min(lengths)}/{max(lengths)} tokens")
    
    def __len__(self) -> int:
        """Retourne le nombre de s√©quences dans le dataset."""
        return len(self.sequences)
    
    def __getitem__(self, idx: int) -> dict:
        """
        Retourne une s√©quence et son masque d'attention.
        
        Args:
            idx (int): Index de la s√©quence
            
        Returns:
            dict: Dictionnaire contenant:
                - 'input_ids': Tensor [seq_len] des tokens d'entr√©e
                - 'attention_mask': Tensor [seq_len] masque (1 = token r√©el, 0 = padding)
                - 'labels': Tensor [seq_len] des tokens de sortie (d√©cal√©s de 1)
                
        Note:
            Pour l'entra√Ænement causal, input_ids = s√©quence[:-1] et labels = s√©quence[1:]
            Cela permet au mod√®le d'apprendre √† pr√©dire le token suivant.
        """
        # R√©cup√©rer la s√©quence
        sequence = self.sequences[idx].copy()
        seq_len = len(sequence)
        
        # Si la s√©quence est trop longue, la tronquer
        if seq_len > self.max_seq_len:
            sequence = sequence[:self.max_seq_len]
            seq_len = self.max_seq_len
        
        # Cr√©er le masque d'attention (1 pour les tokens r√©els, 0 pour le padding)
        attention_mask = [1] * seq_len
        
        # Padding si n√©cessaire
        if seq_len < self.max_seq_len:
            padding_length = self.max_seq_len - seq_len
            sequence = sequence + [self.pad_token_id] * padding_length
            attention_mask = attention_mask + [0] * padding_length
        
        # Convertir en tensors
        input_ids = torch.tensor(sequence, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)
        
        # Pour l'entra√Ænement causal: input = tokens[:-1], target = tokens[1:]
        # Le mod√®le apprend √† pr√©dire le token suivant
        input_ids_shifted = input_ids[:-1]
        labels = input_ids[1:]
        attention_mask_shifted = attention_mask[:-1]
        
        return {
            'input_ids': input_ids_shifted,        # [seq_len-1]
            'attention_mask': attention_mask_shifted,  # [seq_len-1]
            'labels': labels                        # [seq_len-1]
        }
    
    def get_vocab_size(self) -> int:
        """
        Estime la taille du vocabulaire √† partir des s√©quences.
        
        Returns:
            int: Taille du vocabulaire (max token ID + 1)
        """
        max_token = max(max(seq) for seq in self.sequences)
        return max_token + 1


def create_dataloaders(
    train_sequences_path: Path,
    val_sequences_path: Path,
    batch_size: int = 8,
    max_seq_len: int = 1024,
    num_workers: int = 4,
    pin_memory: bool = True
) -> tuple:
    """
    Cr√©e les DataLoaders pour l'entra√Ænement et la validation.
    
    Args:
        train_sequences_path: Chemin vers les s√©quences d'entra√Ænement
        val_sequences_path: Chemin vers les s√©quences de validation
        batch_size: Taille des batchs
        max_seq_len: Longueur maximale des s√©quences
        num_workers: Nombre de workers pour le chargement
        pin_memory: Utiliser pin_memory pour GPU
        
    Returns:
        tuple: (train_loader, val_loader)
    """
    from torch.utils.data import DataLoader
    
    # Cr√©er les datasets
    train_dataset = MIDIDataset(
        sequences_path=train_sequences_path,
        max_seq_len=max_seq_len
    )
    
    val_dataset = MIDIDataset(
        sequences_path=val_sequences_path,
        max_seq_len=max_seq_len
    )
    
    # Cr√©er les dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # M√©langer les donn√©es d'entra√Ænement
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True  # √âviter les derniers batchs incomplets
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,  # Pas de shuffle pour la validation
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False
    )
    
    print(f"\n‚úì DataLoaders cr√©√©s:")
    print(f"  Train: {len(train_dataset)} s√©quences, {len(train_loader)} batchs")
    print(f"  Val:   {len(val_dataset)} s√©quences, {len(val_loader)} batchs")
    
    return train_loader, val_loader


# Test du module
if __name__ == "__main__":
    """Test rapide du dataset."""
    from pathlib import Path
    
    # Chemins (adapter selon ton setup)
    # Depuis src/data/dataset.py, aller vers src/data/processed/
    PROCESSED_DIR = Path(__file__).parent / "processed"
    train_path = PROCESSED_DIR / "train_sequences.pkl"
    
    if train_path.exists():
        # Cr√©er le dataset
        dataset = MIDIDataset(
            sequences_path=train_path,
            max_seq_len=1024
        )
        
        # Tester un item
        print(f"\nüß™ Test d'un item:")
        item = dataset[0]
        print(f"  input_ids shape: {item['input_ids'].shape}")
        print(f"  attention_mask shape: {item['attention_mask'].shape}")
        print(f"  labels shape: {item['labels'].shape}")
        print(f"  Premiers tokens: {item['input_ids'][:10].tolist()}")
        
        # Vocabulaire
        vocab_size = dataset.get_vocab_size()
        print(f"\n  Taille du vocabulaire: {vocab_size}")
    else:
        print(f"‚ùå Fichier non trouv√©: {train_path}")
        print("   Ex√©cute d'abord le notebook de preprocessing !")